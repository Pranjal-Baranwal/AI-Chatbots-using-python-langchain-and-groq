# AI-Chatbots-using-python-langchain-and-groq
This project focuses on what is difference between 2 chatbots model, what are differences between their output and how are they different.
I ran the test on 2 models - 
Paid Gemini model 
Open Source Groq llama model
When I ran both the projects, I came to find out the folllowing observations:-
Groq's llama was much faster averaging 3.12 seconds for complete output than Gemini averaging 16.6 seconds.
Gemini was giving much more accurate json type output than Groq.
Gemini was needing temperature to be set to around 0.6 or 0.6+ to give random outputs but Groq on default was giving random outputs.
